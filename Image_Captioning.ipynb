{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_Captioning.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMGLbeld8kT1yXQG8jQYeA0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mDohfGQKoqpZ","colab_type":"code","outputId":"bd5a6666-7f65-4f79-f1a2-d2d03384cdfd","executionInfo":{"status":"ok","timestamp":1584087048295,"user_tz":-330,"elapsed":33800,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DxzjXFOqo7PV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"eabc4d40-9dac-4190-aa12-5a0e4217f7ab","executionInfo":{"status":"ok","timestamp":1584087054418,"user_tz":-330,"elapsed":2690,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}}},"source":["from os import listdir\n","from pickle import dump\n","#from keras.applications.vgg16 import VGG16\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","#from keras.applications.vgg16 import preprocess_input\n","from keras.models import Model\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.inception_v3 import preprocess_input\n"," \n","# extract features from each photo in the directory\n","def extract_features(directory):\n","\t# load the model\n","\tmodel = InceptionV3(weights=\"imagenet\")\n","\t# re-structure the model\n","\tmodel.layers.pop()\n","\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n","\t# summarize\n","\tprint(model.summary())\n","\t# extract features from each photo\n","\tfeatures = dict()\n","\tfor name in listdir(directory):\n","\t\t# load an image from file\n","\t\tfilename = directory + '/' + name\n","\t\timage = load_img(filename, target_size=(299, 299))\n","\t\t# convert the image pixels to a numpy array\n","\t\timage = img_to_array(image)\n","\t\t# reshape data for the model\n","\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","\t\t# prepare the image for the VGG model\n","\t\timage = preprocess_input(image)\n","\t\t# get features\n","\t\tfeature = model.predict(image, verbose=0)\n","\t\t# get image id\n","\t\timage_id = name.split('.')[0]\n","\t\t# store feature\n","\t\tfeatures[image_id] = feature.reshape(2048,)\n","\t\tprint('>%s' % name)\n","\treturn features\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"vvaloYWpwofh","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"7PSYDsqppKlc","colab_type":"code","colab":{}},"source":["# extract features from all images\n","directory = '/content/drive/My Drive/Flickr8k_Dataset'\n","features = extract_features(directory)\n","print('Extracted Features: %d' % len(features))\n","# save to file\n","dump(features, open('/content/drive/My Drive/features_inception.pkl', 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvgf45Q9kjTI","colab_type":"code","colab":{}},"source":["# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dCVfSgK42tcg","colab_type":"code","colab":{}},"source":["filename = \"/content/drive/My Drive/Flickr8k.token.txt\"\n","# load descriptions\n","doc = load_doc(filename)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xteubtq-ekw5","colab_type":"code","outputId":"a0d0230a-572c-48fd-e096-39321b174aa7","executionInfo":{"status":"ok","timestamp":1584087149735,"user_tz":-330,"elapsed":1127,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# extract descriptions for images\n","def load_descriptions(doc):\n","\tmapping = dict()\n","\t# process lines\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\tif len(line) < 2:\n","\t\t\tcontinue\n","\t\t# take the first token as the image id, the rest as the description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# remove filename from image id\n","\t\timage_id = image_id.split('.')[0]\n","\t\t# convert description tokens back to string\n","\t\timage_desc = ' '.join(image_desc)\n","\t\t# create the list if needed\n","\t\tif image_id not in mapping:\n","\t\t\tmapping[image_id] = list()\n","\t\t# store description\n","\t\tmapping[image_id].append(image_desc)\n","\treturn mapping\n"," \n","# parse descriptions\n","descriptions = load_descriptions(doc)\n","print('Loaded: %d ' % len(descriptions))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Loaded: 8092 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"spr6EuHYix0k","colab_type":"code","colab":{}},"source":["import string\n","def clean_descriptions(descriptions):\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor i in range(len(desc_list)):\n","\t\t\tdesc = desc_list[i]\n","\t\t\t# tokenize\n","\t\t\tdesc = desc.split()\n","\t\t\t# convert to lower case\n","\t\t\tdesc = [word.lower() for word in desc]\n","\t\t\t# remove punctuation from each token\n","\t\t\tdesc = [w.translate(table) for w in desc]\n","\t\t\t# remove hanging 's' and 'a'\n","\t\t\tdesc = [word for word in desc if len(word)>1]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tdesc = [word for word in desc if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tdesc_list[i] =  ' '.join(desc)\n"," \n","# clean descriptions\n","clean_descriptions(descriptions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yzDl7vkvqEb","colab_type":"code","colab":{}},"source":["# convert the loaded descriptions into a vocabulary of words\n","def to_vocabulary(descriptions):\n","\t# build a list of all description strings\n","\tall_desc = set()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n","\treturn all_desc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHgnD0zkD_27","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QVwTcb_lv3P","colab_type":"code","outputId":"65d8949c-8bb7-4411-aa26-dc47139bb936","executionInfo":{"status":"ok","timestamp":1584030653837,"user_tz":-330,"elapsed":3270875,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# summarize vocabulary\n","vocabulary = to_vocabulary(descriptions)\n","print('Vocabulary Size: %d' % len(vocabulary))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 8763\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9z_a7MoJC4Ra","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_42aEYrnC4Nr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wYJMRDdvqHM","colab_type":"code","colab":{}},"source":["# save descriptions to file, one per line\n","def save_descriptions(descriptions, filename):\n","\tlines = list()\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor desc in desc_list:\n","\t\t\tlines.append(key + ' ' + desc)\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()\n"," \n","# save descriptions\n","save_descriptions(descriptions, '/content/drive/My Drive/descriptions_inception.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYtKd2I06JpA","colab_type":"code","colab":{}},"source":["# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZckgKDp6Ml_H","colab_type":"code","colab":{}},"source":["# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QB1eDiUO6Jsx","colab_type":"code","colab":{}},"source":["from pickle import load\n","# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNq2ciZB6Jlm","colab_type":"code","outputId":"e481c507-4760-43cf-b7a1-affb48c81a86","executionInfo":{"status":"ok","timestamp":1584087252555,"user_tz":-330,"elapsed":2688,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# load training dataset (6K)\n","filename = '/content/drive/My Drive/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/My Drive/descriptions_inception.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","# photo features\n","train_features = load_photo_features('/content/drive/My Drive/features_inception.pkl', train)\n","print('Photos: train=%d' % len(train_features))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n","Descriptions: train=6000\n","Photos: train=6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RQrF8GirCXMa","colab_type":"code","colab":{}},"source":["import keras\n","from keras.preprocessing.text import Tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHKh0ixG0La1","colab_type":"code","colab":{}},"source":["# convert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc\n"," \n","# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer\n"," \n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AAh2x-vb0MNh","colab_type":"code","colab":{}},"source":["from keras.utils import to_categorical\n","import numpy as np\n","# create sequences of images, input sequences and output words for an image\n","def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n","\tX1, X2, y = list(), list(), list()\n","\t# walk through each image identifier\n","\tfor key, desc_list in descriptions.items():\n","\t\t# walk through each description for the image\n","\t\tfor desc in desc_list:\n","\t\t\t# encode the sequence\n","\t\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n","\t\t\t# split one sequence into multiple X,y pairs\n","\t\t\tfor i in range(1, len(seq)):\n","\t\t\t\t# split into input and output pair\n","\t\t\t\tin_seq, out_seq = seq[:i], seq[i]\n","\t\t\t\t# pad input sequence\n","\t\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","\t\t\t\t# encode output sequence\n","\t\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\t\t\t\t# store\n","\t\t\t\tX1.append(photos[key])\n","\t\t\t\tX2.append(in_seq)\n","\t\t\t\ty.append(out_seq)\n","\treturn np.array(X1), np.array(X2), np.array(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uGnozR780MRh","colab_type":"code","colab":{}},"source":["# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn int(max(len(d.split()) for d in lines))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TPpuqwRrHyIH","colab_type":"text"},"source":["#Defining the Model"]},{"cell_type":"code","metadata":{"id":"WqgJmwooYO2r","colab_type":"code","colab":{}},"source":["from keras. models import Model\n","from keras.layers import Dropout,Dense,Input,Embedding,LSTM\n","from keras.layers.merge import add"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gGSiLaqaesPa","colab_type":"code","colab":{}},"source":["# define the captioning model\n","def define_model(vocab_size, max_length):\n","\t# feature extractor model\n","\tinputs1 = Input(shape=(2048,))\n","\tfe1 = Dropout(0.5)(inputs1)\n","\tfe2 = Dense(256, activation='relu')(fe1)\n","\t# sequence model\n","\tinputs2 = Input(shape=(max_length,))\n","\tse1 = Embedding(vocab_size, 200, mask_zero=True)(inputs2)\n","\tse2 = Dropout(0.5)(se1)\n","\tse3 = LSTM(256)(se2)\n","\t# decoder model\n","\tdecoder1 = add([fe2, se3])\n","\tdecoder2 = Dense(256, activation='relu')(decoder1)\n","\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\t# tie it together [image, seq] [word]\n","\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n","\t# summarize model\n","\tprint(model.summary())\n","\treturn model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1hEVzPw-Iu0i","colab_type":"code","outputId":"203d4138-bb7b-4aca-d90b-f4b694248db9","executionInfo":{"status":"ok","timestamp":1584076245790,"user_tz":-330,"elapsed":36012,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["from keras.preprocessing.sequence import pad_sequences\n","# train dataset\n"," \n","# load training dataset (6K)\n","filename = '/content/drive/My Drive/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/My Drive/descriptions_inception.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","# photo features\n","train_features = load_photo_features('/content/drive/My Drive/features_inception.pkl', train)\n","print('Photos: train=%d' % len(train_features))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)\n","# prepare sequences\n","X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n","Descriptions: train=6000\n","Photos: train=6000\n","Vocabulary Size: 7579\n","Description Length: 34\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ubgk5UOZI_FX","colab_type":"code","outputId":"2ed39c8a-a65a-4e81-a9df-d6004bf0f4db","executionInfo":{"status":"ok","timestamp":1584076327893,"user_tz":-330,"elapsed":6144,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# dev dataset\n"," \n","# load test set\n","filename = '/content/drive/My Drive/Flickr_8k.devImages.txt'\n","test = load_set(filename)\n","print('Dataset: %d' % len(test))\n","# descriptions\n","test_descriptions = load_clean_descriptions('/content/drive/My Drive/descriptions_inception.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))\n","# photo features\n","test_features = load_photo_features('/content/drive/My Drive/features_inception.pkl', test)\n","print('Photos: test=%d' % len(test_features))\n","# prepare sequences\n","X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset: 1000\n","Descriptions: test=1000\n","Photos: test=1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GV8k6BhbO6-D","colab_type":"code","outputId":"06998196-ab0e-4f37-d7fc-7d936033f5b9","executionInfo":{"status":"ok","timestamp":1584080740236,"user_tz":-330,"elapsed":1506,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["# define the model\n","model = define_model(vocab_size, max_length)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            (None, 34)           0                                            \n","__________________________________________________________________________________________________\n","input_6 (InputLayer)            (None, 2048)         0                                            \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, 34, 200)      1515800     input_7[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 2048)         0           input_6[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 34, 200)      0           embedding_3[0][0]                \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 256)          524544      dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","lstm_3 (LSTM)                   (None, 256)          467968      dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 256)          0           dense_7[0][0]                    \n","                                                                 lstm_3[0][0]                     \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 256)          65792       add_3[0][0]                      \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 7579)         1947803     dense_8[0][0]                    \n","==================================================================================================\n","Total params: 4,521,907\n","Trainable params: 4,521,907\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K6KHGU31io01","colab_type":"code","outputId":"0954a221-408a-4ab7-ece3-c21c66f67c87","executionInfo":{"status":"ok","timestamp":1584081151539,"user_tz":-330,"elapsed":407541,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["# fit model\n","model.fit([X1train, X2train], ytrain, epochs=4, batch_size=512, verbose=1, validation_data=([X1test, X2test], ytest))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 306404 samples, validate on 50903 samples\n","Epoch 1/4\n","306404/306404 [==============================] - 105s 343us/step - loss: 4.7022 - val_loss: 3.9329\n","Epoch 2/4\n","306404/306404 [==============================] - 100s 328us/step - loss: 3.7109 - val_loss: 3.6400\n","Epoch 3/4\n","306404/306404 [==============================] - 100s 327us/step - loss: 3.3751 - val_loss: 3.5271\n","Epoch 4/4\n","306404/306404 [==============================] - 100s 325us/step - loss: 3.1498 - val_loss: 3.4805\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fcb67fa21d0>"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"WAXTDuiyr3kV","colab_type":"code","colab":{}},"source":["model.save(\"/content/drive/My Drive/image_captioning_model_inception_2.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeLIlYMKmEFz","colab_type":"code","outputId":"83c141ee-3b7d-4d0d-b355-ae6905b8afcc","executionInfo":{"status":"ok","timestamp":1583913624246,"user_tz":-330,"elapsed":2459,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# load training dataset (6K)\n","filename = '/content/drive/My Drive/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('/content/drive/My Drive/descriptions_inception.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","# save the tokenizer\n","dump(tokenizer, open('/content/drive/My Drive/tokenizer.pkl', 'wb'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n","Descriptions: train=6000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MDuP6ECkuA9x","colab_type":"text"},"source":["#Generate Dscription from new Image"]},{"cell_type":"code","metadata":{"id":"3MGEfPA0mEMU","colab_type":"code","colab":{}},"source":["# load the tokenizer\n","tokenizer = load(open('/content/drive/My Drive/tokenizer.pkl', 'rb'))\n","# pre-define the max sequence length (from training)\n","max_length = 34"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUDuQ6BZnDqH","colab_type":"code","outputId":"2f60a3e2-1132-40b2-a1c0-989d6bd10288","executionInfo":{"status":"ok","timestamp":1583913672150,"user_tz":-330,"elapsed":1184,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<keras_preprocessing.text.Tokenizer object at 0x7f1880ce1c88>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"36DZ5LNvmEKY","colab_type":"code","colab":{}},"source":["# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D4ksxtCJmEDm","colab_type":"code","colab":{}},"source":["# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","\t# seed the generation process\n","\tin_text = 'startseq'\n","\t# iterate over the whole length of the sequence\n","\tfor i in range(max_length):\n","\t\t# integer encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad input\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\n","\t\t# convert probability to integer\n","\t\tyhat = np.argmax(yhat)\n","\t\t# map integer to word\n","\t\tword = word_for_id(yhat, tokenizer)\n","\t\t# stop if we cannot map the word\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append as input for generating the next word\n","\t\tin_text += ' ' + word\n","\t\t# stop if we predict the end of the sequence\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\treturn in_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEcq9TjZmEA3","colab_type":"code","outputId":"8539f1cd-bf95-44bb-9f17-299cadd68b03","executionInfo":{"status":"ok","timestamp":1584080276841,"user_tz":-330,"elapsed":34155,"user":{"displayName":"ABHISHEK MISHRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghz29Ad-MzDjROoa7znE4aePtKgDW-fX1-kjuH3=s64","userId":"05990114848539160033"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["from keras.models import load_model\n","# extract features from each photo in the directory\n","def extract_features(filename):\n","\t# load the model\n","\tmodel = InceptionV3()\n","\t# re-structure the model\n","\tmodel.layers.pop()\n","\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n","\t# load the photo\n","\timage = load_img(filename, target_size=(299, 299))\n","\t# convert the image pixels to a numpy array\n","\timage = img_to_array(image)\n","\t# reshape data for the model\n","\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","\t# prepare the image for the VGG model\n","\timage = preprocess_input(image)\n","\t# get features\n","\tfeature = model.predict(image, verbose=0)\n","\treturn feature\n","\n","\n","# load the tokenizer\n","tokenizer = load(open('/content/drive/My Drive/tokenizer.pkl', 'rb'))\n","# pre-define the max sequence length (from training)\n","max_length = 34\n","# load the model\n","model = load_model(\"/content/drive/My Drive/image_captioning_model_inception_1.h5\")\n","# load and prepare the photograph\n","photo = extract_features('/content/drive/My Drive/mudila_1.jpg')\n","# generate description\n","description = generate_desc(model, tokenizer, photo, max_length)\n","\n","description=description.split()\n","output_text=\" \".join(description[1:-1])\n","print(output_text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n","\n","Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n","96116736/96112376 [==============================] - 1s 0us/step\n","man wearing black shirt and black pants is sitting on concrete curb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9sSZXVpKmD4B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}